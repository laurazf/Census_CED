{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse as ulp\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "host=\"web5.ces.census.gov\",\n",
    "user=##,\n",
    "password=##,\n",
    "database=\"pmt\"\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "sql = \"SET collation_connection = 'utf8_general_ci'\"\n",
    "mycursor.execute(sql)\n",
    "mydb.commit()\n",
    "sql1 = \"ALTER TABLE api_base CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci\"\n",
    "mycursor.execute(sql1)\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://api.base-search.net/cgi-bin/BaseHttpSearchInterface.fcgi?func=PerformSearch&query=%22Ronald%20Jarmin%22\n",
    "import requests\n",
    "import json\n",
    "import urllib.parse as ulp\n",
    "import urllib.request as ulr\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time\n",
    "\n",
    "class Base:\n",
    "        \n",
    "    def sendToDB(self, proj_id, results, hits):\n",
    "        #print(proj_id)\n",
    "        mydb = mysql.connector.connect(\n",
    "        host=\"web5.ces.census.gov\",\n",
    "        user=\"##\",\n",
    "        password=\"##\",\n",
    "        database=\"pmt\"\n",
    "        )\n",
    "\n",
    "        mycursor = mydb.cursor()\n",
    "        \n",
    "        numPrint = 100\n",
    "        if(hits < 100):\n",
    "            numPrint = hits\n",
    "        '''every quarter should add only new entries and update existing db entries\n",
    "        if they changed-- use doc_id to ID which publications already in db'''\n",
    "        #print(\"numPrint = \" + str(numPrint))\n",
    "        for i in range(numPrint):\n",
    "            title = str(results[1]['response']['docs'][i].get('dctitle'))\n",
    "            #print(title)\n",
    "            authors = results[1]['response']['docs'][i].get('dcperson')\n",
    "            if results[1]['response']['docs'][i].get('dcdoi') is not None:\n",
    "                doi = results[1]['response']['docs'][i].get('dcdoi')[0]\n",
    "            else: \n",
    "                doi = \"None\"\n",
    "            if results[1]['response']['docs'][i].get('dcdate') is not None:\n",
    "                pub_year = str(results[1]['response']['docs'][i].get('dcdate')[:4])\n",
    "                pub_month = str(results[1]['response']['docs'][i].get('dcdate')[5:7])\n",
    "            else:\n",
    "                pub_year = \"None\"\n",
    "                pub_month = \"None\"\n",
    "            full_text_link = str(results[1]['response']['docs'][i].get('dcidentifier'))\n",
    "            publisher = str(results[1]['response']['docs'][i].get('dcpublisher'))\n",
    "            abstract = str(results[1]['response']['docs'][i].get('dcdescription'))\n",
    "            doc_id = str(results[1]['response']['docs'][i].get('dcdocid'))\n",
    "            #print(description)\n",
    "            \n",
    "            sql_dup = \"SELECT * FROM api_base WHERE doc_id = %s AND proj_id = %s\"\n",
    "            data = (doc_id, proj_id)\n",
    "            mycursor.execute(sql_dup, data)\n",
    "            duplicates = mycursor.fetchall()\n",
    "            #print(duplicates)\n",
    "            \n",
    "            \n",
    "            #stop getting rid of duplicates!\n",
    "            if len(duplicates) == 0:\n",
    "                sql = \"INSERT INTO api_base (proj_id, doi, title, pub_year, pub_month, publisher, abstract, full_text_link, doc_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "                val = (proj_id, doi, title, pub_year, pub_month, publisher, abstract, full_text_link, doc_id)\n",
    "                mycursor.execute(sql, val)\n",
    "                pub_id = mycursor.lastrowid\n",
    "                print(\"pub_id: \" +str(pub_id))\n",
    "\n",
    "                mydb.commit()\n",
    "\n",
    "                print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "                if bool(authors):\n",
    "                    for j in range(len(authors)):\n",
    "                        sql_part = \"INSERT INTO api_base_participants (pub_id, proj_id, fullname) VALUES (%s, %s, %s)\"\n",
    "                        val_part = (pub_id, proj_id, authors[j])\n",
    "                        #sql_part = \"INSERT INTO api_base_participants_test (proj_id, fullname) VALUES (%s, %s)\"\n",
    "                        #val_part = (proj_id, authors[j])\n",
    "                        mycursor.execute(sql_part, val_part)\n",
    "\n",
    "                        mydb.commit()\n",
    "\n",
    "                        print(mycursor.rowcount, \"participant record inserted.\")\n",
    "                    \n",
    "            else: '''if duplicates in same query, 1st prob has most info. but if queries from separate quarters, prob want newer one-- how to logic?'''\n",
    "                print(\"duplicate value, not added\")\n",
    "        \n",
    "        \n",
    "        sql_update_year = \"UPDATE api_base SET pub_year=NULL WHERE pub_year='None'\"\n",
    "        mycursor.execute(sql_update_year)\n",
    "        mydb.commit()\n",
    "        sql_update_month = \"UPDATE api_base SET pub_month=NULL WHERE pub_month='None'\"\n",
    "        mycursor.execute(sql_update_month)\n",
    "        mydb.commit()\n",
    "        sql_update_doi = \"UPDATE api_base SET doi=NULL WHERE doi='None'\"\n",
    "        mycursor.execute(sql_update_doi)\n",
    "        mydb.commit()\n",
    "        sql_update_title = \"UPDATE api_base SET title=NULL WHERE title='None'\"\n",
    "        mycursor.execute(sql_update_title)\n",
    "        mydb.commit()\n",
    "        sql_update_link = \"UPDATE api_base SET full_text_link=NULL WHERE full_text_link='None'\"\n",
    "        mycursor.execute(sql_update_link)\n",
    "        mydb.commit()\n",
    "        sql_update_publisher = \"UPDATE api_base SET publisher=NULL WHERE publisher='None'\"\n",
    "        mycursor.execute(sql_update_publisher)\n",
    "        mydb.commit()\n",
    "        sql_update_abstract = \"UPDATE api_base SET abstract=NULL WHERE abstract='None'\"\n",
    "        mycursor.execute(sql_update_abstract)\n",
    "        mydb.commit()\n",
    "        sql_update_id = \"UPDATE api_base SET doc_id=NULL WHERE doc_id='None'\"\n",
    "        mycursor.execute(sql_update_id)\n",
    "        mydb.commit()\n",
    "        print(\"changed None values to null in doi, title, year, month, full text link, publisher, abstract doc id\")\n",
    "        \n",
    "class BaseSearch(Base):\n",
    "            \n",
    "    def search(self, _path, proj_id, _query):\n",
    "        header = [\"Accept: text/xml\"]\n",
    "        url = \"https://api.base-search.net/cgi-bin/BaseHttpSearchInterface.fcgi\"\n",
    "        link = url+_path+\"&query=\"+_query+\"&hits=100&format=json\"\n",
    "        \n",
    "        request = requests.get(link, proxies={'https':'##'})      \n",
    "        \n",
    "        if (request.status_code == 200):\n",
    "            data = [proj_id, request.json()]\n",
    "            return data\n",
    "    \n",
    "    '''change so do not need to input author name to run, check all aut names'''\n",
    "    def searchAll(self, proj_id, _query):\n",
    "        return self.search(\"?func=PerformSearch\", proj_id, _query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "altered auto-increment\n"
     ]
    }
   ],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "        host=\"web5.ces.census.gov\",\n",
    "        user=\"##\",\n",
    "        password=\"##\",\n",
    "        database=\"pmt\"\n",
    "        )\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "test = \"ALTER TABLE api_base AUTO_INCREMENT = 1\"\n",
    "mycursor.execute(test)\n",
    "mydb.commit()\n",
    "print(\"altered auto-increment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BaseSearch()\n",
    "results = api.searchAll(1360, '\"Ann Owens\"')\n",
    "hits = results[1]['response']['numFound']\n",
    "print(\"Total hits: \" + str(hits))\n",
    "print(\"proj_id: \" +str(results[0]))\n",
    "proj_id = str(results[0])\n",
    "print(results)\n",
    "#api.sendToDB(proj_id, results, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tika\n",
    "# from tika import parser # pip install tika\n",
    "# raw = parser.from_file('abstract_5.pdf')\n",
    "# print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fang0311\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.set_proxy('http://proxy.tco##')\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "api_key = ##\n",
    "\n",
    "headers = { 'Authorization' : 'key=%s' %  (api_key).decode(\"ascii\") }\n",
    "\n",
    "#https://www.ces.census.gov/api/iaa/v1.3/project/status/CP?fields=cms_project_id,title&page=0&limit=10\n",
    "def run_query(path, fields, page, limit):\n",
    "    request = requests.get('https://www.ces.census.gov/api/iaa/v1.3%s?fields=%s&page=%s&limit=%s' % (path,fields,page,limit), headers=headers, proxies={\"https\" : \"148.129.75.18:3128\"})\n",
    "    if request.status_code == 200:\n",
    "        return request.json()\n",
    "    else:\n",
    "        raise Exception(\"Query failed to run by returning code of {}.\".format(request.status_code))\n",
    "\n",
    "def name_and_ds(proj_id): \n",
    "    data = {}\n",
    "    proj = run_query('/project/'+str(proj_id),'cms_project_id,title','0','10') # Execute the project query\n",
    "    #'/project/status/CP'\n",
    "    full = proj[0]['title']\n",
    "    parts = full.split(\"-\")\n",
    "    data['title']= parts[2][1:]\n",
    "    #print(proj['cms_project_id'])\n",
    "    datasets = run_query('/project/%s/data' % proj[0]['cms_project_id'],'name','0','10') # Execute the dataset query\n",
    "    sets = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        sets.append(ds['name'])\n",
    "\n",
    "    data['datasets']=sets\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Impact of Foreign Trade on the U.S. Economy',\n",
       " 'datasets': ['Annual Survey of Manufactures',\n",
       "  'Census of Manufactures',\n",
       "  'Census of Mining',\n",
       "  'Census of Construction Industries',\n",
       "  'Enterprise Summary Report - ES9100 (large company)',\n",
       "  'Auxiliary Establishment - ES9200',\n",
       "  'Standard Statistical Establishment Listing ',\n",
       "  'Census of Retail Trade',\n",
       "  'Census of Services',\n",
       "  'Census of Wholesale Trade']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_and_ds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from base64 import b64encode\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "api_key = ##\n",
    "\n",
    "headers = { 'Authorization' : 'key=%s' %  (api_key).decode(\"ascii\") }\n",
    "\n",
    "def run__abstract_query(path,fields,page,limit):\n",
    "    request = requests.get('https://www.ces.census.gov/api/iaa/v1.3%s?fields=%s&page=%s&limit=%s' % (path,fields,page,limit), headers=headers, proxies={\"https\" : \"148.129.75.18:3128\"})\n",
    "    if request.status_code == 200:\n",
    "        return request.json()\n",
    "    else:\n",
    "        raise Exception(\"Query failed to run by returning code of {}.\".format(request.status_code))\n",
    "        \n",
    "def get_abstract(projid):\n",
    "    results = run__abstract_query('/project/'+str(projid)+'/proposal/text','mime_data','0','10')\n",
    "    #print(results)\n",
    "    return results[0]['mime_data'].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_abstract(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks\n",
      "{'1': ['Uncertainty', 'exception', 'data', 'difficult', 'technology', 'cause', 'uncertainty', 'effect', 'uncertainty', 'response', 'necessary', 'analysis', 'employment', 'investment', 'research', 'development', 'productivity', 'project', 'project', 'quality', 'volatility', 'external', 'volatility', 'examination', 'assessment', 'aspect', 'quality', 'improvement', 'aggregate', 'data', 'analysis', 'significance', 'dispersion', 'project', 'recommendation', 'aggregate', 'yearly', 'proposal', 'interpretation', 'example', 'extent', 'volatility', 'recession', 'contribution', 'course', 'recession', 'non-response', 'volatility', 'volatility', 'cycle', 'project', 'cyclical', 'industrial', 'non-response', 'project', 'Research', 'time', 'volatility', 'example', 'industry', 'recession', 'population', 'amount', 'imputed', 'project', 'quality', 'many', 'Correspondence', 'literature', 'exception', 'difficult', 'large', 'ask', 'large', 'important', 'monetary', 'unanswered', 'technology', 'cause', 'role', 'uncertainty', 'idea', 'uncertainty', 'countercyclical', 'uncertainty', 'business', 'new', 'economic', 'argument', 'role', 'uncertainty', 'theory', 'business', 'evidence', 'uncertainty', 'scarce', 'behavior', 'dispersion', 'much', 'understood', 'time', 'uncertainty', 'literature', 'setup', 'few', 'uncertainty', 'exception', 'investment', 'energy', 'presence', 'construction', 'measure', 'detail', 'uncertainty', 'investment', 'cautious', 'paper', 'progress', 'model', 'heterogeneous', 'uncertainty', 'large', 'employment', 'uncertainty', 'many', 'investment', 'hiring', 'reallocation', 'output', 'low', 'data', 'large', 'show', '%', '%', 'aggregate', 'due', 'reallocation', 'uncertainty', 'large', 'measurement', 'uncertainty', 'counter-cyclical', 'al', 'longrun', 'volatility', 'public', 'owned', 'SUMMARY', 'project', 'literature', 'large', 'role', 'uncertainty', 'business', 'project', 'interlinked', 'analysis', 'Section', 'rise', 'uncertainty', 'data', 'dispersion', 'volatility', 'quantification', 'size', 'significance', 'uncertainty', 'order', 'evidence', 'firm', 'industry', 'macro', 'measurement', 'dispersion', 'volatility', 'data', 'Section', 'impact', 'uncertainty', 'productivity', 'uncertainty', 'responsiveness', 'economic', 'cautious', 'employment', 'investment', 'responsive', 'evidence', 'responsiveness', 'productivity', 'demand', 'Section', 'size', 'level', 'technology', 'uncertainty', 'model', 'report', 'reallocation', 'entry', 'exit', '%', 'manufacturing', '%', 'fact', 'reallocation', 'suggest', 'important', 'impact', 'uncertainty', 'evidence', 'large', 'unobservable', 'observable', 'economic', 'result', 'uncertainty', 'time', 'exception', 'al', 'long-run', 'public', 'remainder', 'section', 'approach', 'construct', 'hypothesis', 'annual', 'aggregate', 'volatility', 'investment', 'labor', 'total-factor', 'productivity', 'following', 'methodology', 'Employment', 'employment', 't', 't-xi', 't-1', '/', 'xi', 't-1', 'g', 'x', 'variable', 'employment', 'case', 'measure', 'advantage', 'symmetric', 'entry', 'exit', 'Uncertainty', 'dispersion', 'volatility', 'unweighted', 'employment', 'growth', 'year.5', 'volatility', 'establishment', 'window', 'period', 'unweighted', 'employment', 'population', 'basis', 'continuous', 'detailed', 'size', 'sample', 'order', 'possibility', 'differential', 'measure', 'measure', 'al', 'Employment', 'various', 'year', 'observation', 'example', 'average', 'year', 'analysis', 'measure', 'detail', 'al', 'number', 'additional', 'measure', 'skewness', 'kurtosis', 'source', 'level', 'forward', 'length', 'series', 'sample', 'year', 'continuous', 'dispersion', 'volatility', 'employment', 'growth', 'unweighted', 'employment', 'dispersion', 'volatility', 'course', 'disadvantage', 'database', 'non-random', 'size', 'probability', 'database', 'volatility', 'impact', 'uncertainty', 'disadvantage', 'manufacturing', 'sample', 'whole', 'sample', 'data', 'uncertainty', 'industry', 'macro', 'sample', 'productivity', 'construct', 'output', 'employment', 'labor', 'energy', 'capital', 'combined', 'capital', 'error', 'sample', 'year', 'available', 'late', 'sufficient', 'productivity', 'several', 'productivity', 'data', 'primary', 'output', 'employee', 'cost-share', 'dispersion', 'volatility', 'same', 'employment', 'first', 'growth', 'productivity', 'unweighted', 'establishment', 'level', 'possible', 'level', 'example', 'analysis', 'section', 'aggregated', 'level', 'al', '06-17', 'information', 'revenue', 'employment', 'dispersion', 'volatility', 'investment', 'careful', 'size', 'addition', 'dispersion', 'volatility', 'research', 'development', 'database', 'information', 'research', 'development', 'establishment', 'year', 'sample', 'major', 'performing', 'measure', 'growth', 'dispersion', 'volatility', 'employment', 'investment', 'productivity', 'utilization', 'dispersion', 'volatility', 'proxy', 'variance', 'information', 'operational', 'capital', 'manufacturing', 'annual', 'biennial', 'basis', 'measure', 'growth', 'dispersion', 'volatility', 'employment', 'investment', 'productivity', 'uncertainty', 'share', 'economy', 'recession', 'year', 'timing', 'committee', 'constant', 'variable', 'b0', 'b1', 'uncertainty', 'recession', 'ratio', 'coefficient', 'variable', 'constant', 'b1/b0', 'estimate', 'correlation', 'uncertainty', 'real', 'growth', 'cyclicality', 'uncertainty', 'example', 'b1/b0', 'one-third', 'recessionary', 'non-recessionary', 'volatility', 'range', 'alternative', 'firm', 'industry', 'uncertainty', 'Table', 'briefly', 'quarterly', 'uncertainty', 'uncertainty', 'Figure', 'index', 'grey', 'graph', 'external', 'Section', 'easy', 'time', 'external', 'volatility', 'industry', 'example', 'mid', 'data', 'dispersion', 'absent', 'data', 'variance', 'subset', 'data', 'examination', 'likely', 'understanding', 'quality', 'improvement', 'uncertainty', 'cross', 'firm', 'growth', 'Table', 'employment', 'investment', 'only', 'uncertainty', 'volatility', 'industry', 'straightforward', 'significant', 'example', 'industry', 'measure', 'uncertainty', 'case', 'key', 'increase', 'volatility', 'individual', 'Section', 'volatility', 'data', 'main', 'quality', 'dispersion', 'Table', 'measure', 'individual', 'consistency', 'stock', 'addition', 'data', 'helpful', 'uncertainty', 'recession', 'increase', 'increase', 'dispersion', 'employment', 'stock', 'survey', 'company', 'increase', 'dispersion', 'point', 'al', 'spread', 'employment', 'due', 'issue', 'volatility', 'al', 'accuracy', 'uncertainty', 'Table', 'useful', 'source', 'data', 'industry', 'data', 'data', 'noted', 'independent', 'Hence', 'data', 'error', 'series', 'real', 'forecast', 'Table', 'strong', 'uncertainty', 'high', 'cautious', 'productivity', 'response', 'response', 'fall', 'responsiveness', '∂2ΔEt', 'ΔEt', 'change', 'productivity', 'change', 'uncertainty', 'Figure', 'idea', 'different', 'uncertainty', 'responsive', 'high', 'uncertainty', 'percentile', 'low', 'uncertainty', 'percentile', 'model', 'panel', 'responsive', 'uncertainty', 'consistent', 'uncertainty', 'large', 'demand', '%', 'uncertainty', 'equivalent', 'uncertainty', 'recession', 'component', 'project', 'data', 'impact', 'uncertainty', 'responsiveness', 'productivity', 'regression', 'responsiveness', 'productivity', 'different', 'b2σt', 'b3ΔAitσt', 'b4Xit', 'eit', 'ΔEit', 'change', 'growth', 'productivity', 'σt', 'measure', 'uncertainty', 'industry', 'size', 'age', 'year', 'model', 'b3', 'consistent', 'prediction', 'uncertainty', 'response', 'productivity', 'course', 'number', 'specification', 'i', 'expenditure', 'dependent', 'response', 'capital', 'uncertainty', 'ii', 'indicator', 'number', 'recession', 'year', 'order', 'responsiveness', 'productivity', 'iii', 'firm', 'industry', 'iv', 'output', 'productivity', 'regression', 'demand', 'calculation', 'productivity', 'v', 'order', 'regression', 'other', 'non-linear', 'vi', 'variety', 'estimation', 'such', 'within-groups', 'Blundell-Bond', 'endogeneity', 'investment', 'slope', 'relationship', 'investment', 'various', 'uncertainty', 'spline', 'example', 'sample', 'normal', 'recessionary', 'plot', 'investment', 'model', 'responsiveness', 'investment', 'productivity', 'Note', 'line', 'function', 'graph', 'establishment', 'non-parametric', 'bootstrapping', 'number', 'different', 'i', 'investment', 'employment', 'ii', 'sample', 'recession/non-recession', 'factor', 'factor', 'cost-share', 'and/or', 'approach', 'output', 'factor', 'different', 'regression', 'residual', 'measure', 'productivity', 'investment', 'indicator', 'productivity', 'iii', 'iv', 'example', 'other', 'size', 'age', 'goal', 'project', 'question', 'extent', 'intuition', 'impact', 'uncertainty', 'uncertainty', 'adjustment', 'example', 'expensive', 'resale', 'capital', 'example', 'uncertainty', 'high', 'investment', 'disinvestment', 'course', 'firm', 'economy', 'cautious', 'investment', 'recession', 'uncertainty', 'caution', 'uncertainty', 'literature', 'ignored', 'productivity', 'uncertainty', 'time', 'particular', 'uncertainty', 'Section', 'significant', 'heterogeneous', 'first', 'capital', 'labor', 'employment', 'costless', 'form', 'host', 'practice', 'instance', 'effort', 'capital', 'disruption', 'new', 'specific', 'size', 'number', 'robustness', 'standard', 'literature', 'subject', 'productivity', 'aggregate', 'productivity', 'time', 'productivity', 'small', 'normal', 'large', 'uncertainty', 'high', 'nonlinear', 'decision', 'possible', 'economy', 'model', 'uncertainty', 'large', 'model', 'able', 'uncertainty', 'Section', 'large', 'importance', 'volatility', 'data', 'response', 'monetary', 'prediction', 'model', 'responsiveness', 'cautious', 'react', 'shock', 'impulse', 'model', 'unresponsive', 'interest', 'tax', 'combination', 'model', 'light', 'volatility', 'number', 'volatility', 'example', 'model', 'question', 'impact', 'trend', 'volatility', 'DATA', 'project', 'full', 'number', 'recessionary', 'analysis', 'establishment', 'order', 'precision', 'volatility', 'Longitudinal', '1976-2011', 'Annual', '1973-2011', 'Census', '1963-2011', '1972-2011', '1974-2011', '1974-2011', 'Compustat-SSEL', 'addition', 'uncertainty', 'non-census', 'data', 'online', 'https', '//wrds.wharton.upenn.edu', 'Access', 'data', 'subscription', 'data', 'http', '//www.federalreserve.gov/releases/g17', 'data', 'http', '//www.cboe.com/micro/vix/historical.aspx', 'data', 'online', 'https', '//wrds.wharton.upenn.edu', 'subscription', 'data', 'http', '//www.philadelphiafed.org/econ/spf/spffind.html', 'Necessary', 'Project', 'main', 'provide', 'size', 'data', 'data', 'volatility', 'time', 'comparison', 'time-series', 'uncertainty', 'sensitivity', 'census', 'important', 'year', 'able', 'concern', 'variation', 'cross', 'sectional', 'due', 'common', 'census', 'public', 'private', 'domestic', 'foreign', 'database', 'contrast', 'listing', 'exchange', 'foreign', 'overseas', 'public', 'international', 'current', '%', 'difference', 'considerable', 'monetary', 'national', 'employment', 'inflation', 'data', 'interest', 'series', 'long', 'important', 'early', 'database', 'coverage', 'mid', 'expansion', 'mid', 'selection', 'construction', 'firmlevel', 'panel', 'early', 'number', 'quarterly', 'investment', 'available', 'analysis', 'Risk', 'Disclosure', 'proposal', 'risk', 'disclosure', 'data', 'reason', 'analysis', 'research', 'necessity', 'large', 'uncertainty', 'case', 'case', 'industry-level', 'industry', 'aggregate', 'industry', 'example', 'growth', '4-digit', 'mean', 'Funding', 'funding', 'research', 'grant', 'duration', 'project', 'summer', 'intensity', 'use', 'week', 'analysis', 'Individual', 'vol', '409-429', 'Review', 'Production', '20th', 'Investment', 'Quarterly'], '2': ['economic theory', 'oil price', 'business cycle', 'large negative', 'alternative driver', 'business cycle', 'capacity utilization', 'annual basis', 'predominant purpose', 'likely extent', 'measurement error', 'improved methodology', 'time variation', 'establishment level', 'business cycle', 'data imputation', 'considerable time', 'survey estimation', 'counter-cyclical volatility', 'same confidence', 'response rate', 'cyclical bias', 'standard Real-Business', 'oil price', 'negative technology', 'seminal paper', 'alternative explanation', 'business cycle', 'fiscal policy', 'major question', 'business cycle', 'large negative', 'alternative explanation', 'business cycle', 'general idea', 'investor sentiment', 'so-called animal', 'large role', 'business cycle', 'first moment', 'second moment', 'standard analytical', 'numerical solution', 'single firm', 'efficient capital', 'oil-price uncertainty', 'partial equilibrium', 'Recent work', 'modeling side', 'stochastic volatility', 'aggregate output', 'productivity growth', 'high productivity', 'Recent research', 'productivity growth', 'productivity growth', 'time variation', 'time series', 'establishment-level employment', 'productivity growth', 'industry volatility', 'other uncertainty', 'establishment responsiveness', 'high uncertainty', 'high uncertainty', 'first moment', 'second moment', 'full contribution', 'establishment reallocation', 'product switching', 'extent uncertainty', 'perfect way', 'exact measure', 'business cycle', 'establishment-level employment', 'employment growth', 't+0.5 xi', 'growth rate', 'cross-sectional basis', 'weighted dispersion', 'standard deviation', 'interquartile range', '90-10 spread', 'ten consecutive', 'weighted mean', 'above exercise', 'consistent aggregation', 'detailed discussion', 'establishment level', 'interested reader', 'industry level', 'firm level', 'large sample', 'exact same', 'time series', 'careful comparison', 'sample selection', 'external firm', 'second way', 'potential bias', 'large-establishment manufacturing', 'capital stock', 'capital stock', 'first few', 'capital stock', 'total-factor productivity', 'time series', 'Undertaking analysis', 'aggregated data', 'ultimate parent', 'population census', 'stratified sample', 'capacity utilization', 'capacity utilization', 'Evaluating Uncertainty', 'new uncertainty', 'counter cyclicality', 'annual variable', 'business cycle', 'percentage rise', 'full year', 'recession share', 'standard error', 'standard error', 'coefficient b1', 'constant b0', 'wide array', 'alternative uncertainty', 'macro level', 'unified measure', 'common scale', 'notable difference', 'time period', 'substantial increase', 'small number', 'strong sign', 'measurement error', 'important aspect', 'alternative way', 'time variation', 'sectional spread', 'Compustat quarterly', 'selection bias', 'previous paragraph', 'common scale', 'time period', 'strong increase', 'parallel increase', 'exact same', 'detailed examination', 'firm-level stock', 'measurement error', 'real uncertainty', 'real variance', 'internal accounts—which', 'Selection bias', 'course none', 'industry level', 'annual frequency', 'time series', 'industry level', 'firm level', 'industry level', 'macroeconomic uncertainty', 'cyclical uncertainty', 'broad crosscheck', 'option value', 'employment growth', 'simulation exercise', 'annual firm-level', 'cautionary effect', 'average response', 'typical increase', 'graphical analysis', 'establishment panel', 'following form', 'b0+ b1ΔAit', 'employment growth', 'other control', 'subscripts i', 'key term', 'coefficient b3', 'investment rate', 'technology investment', 'macro uncertainty', 'growth rate', 'growth rate', 'measurement error', 'interaction term', 'unobserved heterogeneity', 'productivity growth', 'productivity growth', 'productivity growth', 'similar manner', 'non-recessionary line', 'recessionary line', 'average investment', 'productivity growth', 'firm level', 'Point-wise standard', 'regression analysis', 'graphical analysis', 'cost-share approach', 'primary approach', 'industry level', 'Olley-Pakes approach', 'invertibility condition', 'output growth', 'productivity growth', 'semi-parametric estimation', 'establishment industry', 'uncertainty matter', 'business cycle', 'option value', 'macro uncertainty', 'pent-up demand', 'previous period', 'rapid rebound', 'economic activity', 'second moment', 'second moment', 'large number', 'non-convex adjustment', 'capital stock', 'adjustment cost', 'current production', 'certain plant', 'large literature', 'capital adjustment', 'exogenous process', 'idiosyncratic component', 'standard setup', 'second moment', 'optimal decision', 'non-standard environment', 'central research', 'time series', 'high uncertainty', 'high uncertainty', 'low uncertainty', 'desired effect', 'empirical evidence', 'many other', 'economic growth', 'data series', 'free online', 'free online', 'free online', 'sample size', 'territorial coverage', 'sample period', 'large size', 'firm level', 'large sample', 'large sample', '4-digit industry', 'idiosyncratic volatility', 'Territorial coverage', 'complete global', 'domestic US', 'commercial activity', 'data coverage', 'fiscal policy', 'territorial coverage', 'long-run coverage', 'infrequent nature', 'small sample', 'time period', 'aggregate level', 'macro focus', 'establishment level', 'next year', 'desired start', 'Empirical Exploration', 'Real Business'], '3': ['large negative technology', 'reliable time series', 'nbloom @ stanford.edu', 'large negative technology', 'time series variation', 'stylized single-firm economy', 'generate stock market', 'following research question', 'industry level cyclicality', 'general equilibrium model', 'general equilibrium model', 'retail productivity growth', 'ex ante uncertainty', 'little prior literature', 'private sector establishment', 'level employment volatility', 'industry level dispersion', 'aggregate volatility measure', 'individual volatility series', 'time series movement', 'cross-sectional employment growth', 'common macro shock', 'industry level spread', 'perpetual inventory method', 'initial condition assumption', 'simple labor productivity', 'rich micro-data set', 'publicly-quoted firm level', 'labor capacity utilization', 'constructed annual uncertainty', 'establishment level uncertainty', 'annualized aggregate level', 'second firm-level measure', 'time-varying measurement error', 'quarterly industry dispersion', 'cross-sectional industry dispersion', 'positive productivity shock', 'negative productivity shock', 'aggregate uncertainty index', 't index establishment', 'potential spurious correlation', 'other nonparametric regression', 'macro uncertainty indicator', 'total factor productivity', 'structural economic model', 'general equilibrium model', 'common final good', 'low resale value', 'time-varying second moment', 'fiscal policy fall', 'first moment shock', 'high uncertainty versus', 'long-term downward trend', 'own time series', 'accurate time series', 'differential industry level', 'possible sample size', 'average cross-sectional variance'], '4': ['prior firm level evidence', 'tax-paying firm unit level', 'principal component factor method', 'monthly industry level production', 'average annual investment response', 'consistent establishment level database']}\n",
      "most frequent words\n",
      "['data', 'uncertainty', 'measures', 'Census', 'productivity', 'volatility', 'shocks', 'firms', 'using', 'level', 'growth', 'employment', 'industry', 'recessions', 'time', 'large']\n",
      "ngrams\n",
      "{'2': ['Census data'], '3': ['Davis et al']}\n",
      "keywords w/duplicates\n",
      "['Census data', 'economic theory', 'oil price', 'business cycle', 'large negative', 'alternative driver', 'business cycle', 'capacity utilization', 'annual basis', 'predominant purpose', 'likely extent', 'measurement error', 'improved methodology', 'time variation', 'establishment level', 'business cycle', 'data imputation', 'considerable time', 'survey estimation', 'counter-cyclical volatility', 'same confidence', 'response rate', 'cyclical bias', 'standard Real-Business', 'oil price', 'negative technology', 'seminal paper', 'alternative explanation', 'business cycle', 'fiscal policy', 'major question', 'business cycle', 'large negative', 'alternative explanation', 'business cycle', 'general idea', 'investor sentiment', 'so-called animal', 'large role', 'business cycle', 'first moment', 'second moment', 'standard analytical', 'numerical solution', 'single firm', 'efficient capital', 'oil-price uncertainty', 'partial equilibrium', 'Recent work', 'modeling side', 'stochastic volatility', 'aggregate output', 'productivity growth', 'high productivity', 'Recent research', 'productivity growth', 'productivity growth', 'time variation', 'time series', 'establishment-level employment', 'productivity growth', 'industry volatility', 'other uncertainty', 'establishment responsiveness', 'high uncertainty', 'high uncertainty', 'first moment', 'second moment', 'full contribution', 'establishment reallocation', 'product switching', 'extent uncertainty', 'perfect way', 'exact measure', 'business cycle', 'establishment-level employment', 'employment growth', 't+0.5 xi', 'growth rate', 'cross-sectional basis', 'weighted dispersion', 'standard deviation', 'interquartile range', '90-10 spread', 'ten consecutive', 'weighted mean', 'above exercise', 'consistent aggregation', 'detailed discussion', 'establishment level', 'interested reader', 'industry level', 'firm level', 'large sample', 'exact same', 'time series', 'careful comparison', 'sample selection', 'external firm', 'second way', 'potential bias', 'large-establishment manufacturing', 'capital stock', 'capital stock', 'first few', 'capital stock', 'total-factor productivity', 'time series', 'Undertaking analysis', 'aggregated data', 'ultimate parent', 'population census', 'stratified sample', 'capacity utilization', 'capacity utilization', 'Evaluating Uncertainty', 'new uncertainty', 'counter cyclicality', 'annual variable', 'business cycle', 'percentage rise', 'full year', 'recession share', 'standard error', 'standard error', 'coefficient b1', 'constant b0', 'wide array', 'alternative uncertainty', 'macro level', 'unified measure', 'common scale', 'notable difference', 'time period', 'substantial increase', 'small number', 'strong sign', 'measurement error', 'important aspect', 'alternative way', 'time variation', 'sectional spread', 'Compustat quarterly', 'selection bias', 'previous paragraph', 'common scale', 'time period', 'strong increase', 'parallel increase', 'exact same', 'detailed examination', 'firm-level stock', 'measurement error', 'real uncertainty', 'real variance', 'internal accounts—which', 'Selection bias', 'course none', 'industry level', 'annual frequency', 'time series', 'industry level', 'firm level', 'industry level', 'macroeconomic uncertainty', 'cyclical uncertainty', 'broad crosscheck', 'option value', 'employment growth', 'simulation exercise', 'annual firm-level', 'cautionary effect', 'average response', 'typical increase', 'graphical analysis', 'establishment panel', 'following form', 'b0+ b1ΔAit', 'employment growth', 'other control', 'subscripts i', 'key term', 'coefficient b3', 'investment rate', 'technology investment', 'macro uncertainty', 'growth rate', 'growth rate', 'measurement error', 'interaction term', 'unobserved heterogeneity', 'productivity growth', 'productivity growth', 'productivity growth', 'similar manner', 'non-recessionary line', 'recessionary line', 'average investment', 'productivity growth', 'firm level', 'Point-wise standard', 'regression analysis', 'graphical analysis', 'cost-share approach', 'primary approach', 'industry level', 'Olley-Pakes approach', 'invertibility condition', 'output growth', 'productivity growth', 'semi-parametric estimation', 'establishment industry', 'uncertainty matter', 'business cycle', 'option value', 'macro uncertainty', 'pent-up demand', 'previous period', 'rapid rebound', 'economic activity', 'second moment', 'second moment', 'large number', 'non-convex adjustment', 'capital stock', 'adjustment cost', 'current production', 'certain plant', 'large literature', 'capital adjustment', 'exogenous process', 'idiosyncratic component', 'standard setup', 'second moment', 'optimal decision', 'non-standard environment', 'central research', 'time series', 'high uncertainty', 'high uncertainty', 'low uncertainty', 'desired effect', 'empirical evidence', 'many other', 'economic growth', 'data series', 'free online', 'free online', 'free online', 'sample size', 'territorial coverage', 'sample period', 'large size', 'firm level', 'large sample', 'large sample', '4-digit industry', 'idiosyncratic volatility', 'Territorial coverage', 'complete global', 'domestic US', 'commercial activity', 'data coverage', 'fiscal policy', 'territorial coverage', 'long-run coverage', 'infrequent nature', 'small sample', 'time period', 'aggregate level', 'macro focus', 'establishment level', 'next year', 'desired start', 'Empirical Exploration', 'Real Business', 'large negative technology', 'reliable time series', 'nbloom @ stanford.edu', 'large negative technology', 'time series variation', 'stylized single-firm economy', 'generate stock market', 'following research question', 'industry level cyclicality', 'general equilibrium model', 'general equilibrium model', 'retail productivity growth', 'ex ante uncertainty', 'little prior literature', 'private sector establishment', 'level employment volatility', 'industry level dispersion', 'aggregate volatility measure', 'individual volatility series', 'time series movement', 'cross-sectional employment growth', 'common macro shock', 'industry level spread', 'perpetual inventory method', 'initial condition assumption', 'simple labor productivity', 'rich micro-data set', 'publicly-quoted firm level', 'labor capacity utilization', 'constructed annual uncertainty', 'establishment level uncertainty', 'annualized aggregate level', 'second firm-level measure', 'time-varying measurement error', 'quarterly industry dispersion', 'cross-sectional industry dispersion', 'positive productivity shock', 'negative productivity shock', 'aggregate uncertainty index', 't index establishment', 'potential spurious correlation', 'other nonparametric regression', 'macro uncertainty indicator', 'total factor productivity', 'structural economic model', 'general equilibrium model', 'common final good', 'low resale value', 'time-varying second moment', 'fiscal policy fall', 'first moment shock', 'high uncertainty versus', 'long-term downward trend', 'own time series', 'accurate time series', 'differential industry level', 'possible sample size', 'average cross-sectional variance']\n",
      "334\n",
      "final keywords\n",
      "['likely extent', 'high uncertainty', 'standard analytical', 'exact same', 'coefficient b3', 'establishment responsiveness', 'cautionary effect', 'long-run coverage', 'economic activity', 'Real Business', 'labor capacity utilization', 'simulation exercise', 'second firm-level measure', 'Recent research', 'strong sign', 'adjustment cost', 'percentage rise', 'standard error', 'economic growth', 'pent-up demand', 'b0+ b1ΔAit', 'potential spurious correlation', 'large role', 'typical increase', 'large sample', 'major question', 'Territorial coverage', 'stratified sample', 'aggregate uncertainty index', 'interaction term', 'primary approach', 'idiosyncratic component', 'industry level spread', 'establishment panel', 'large number', 'sectional spread', 'constant b0', 'subscripts i', 'rapid rebound', 'data imputation', 'general idea', 'detailed discussion', 'recessionary line', 'commercial activity', 'aggregate level', 'product switching', 'large-establishment manufacturing', 'wide array', 'counter-cyclical volatility', 'ten consecutive', 'extent uncertainty', 'average response', 'business cycle', 'time period', 'domestic US', 'annual firm-level', 'stylized single-firm economy', 'same confidence', 'macroeconomic uncertainty', 'following research question', 'next year', 'private sector establishment', 'perpetual inventory method', 'time-varying second moment', 'counter cyclicality', 'negative productivity shock', 'parallel increase', 'investor sentiment', 'negative technology', 'optimal decision', 'Undertaking analysis', 'little prior literature', 'non-standard environment', 'fiscal policy', 'cross-sectional basis', 'other uncertainty', 'improved methodology', 'weighted dispersion', 'capital adjustment', 'considerable time', 'many other', 'annualized aggregate level', 'other nonparametric regression', 'current production', 'alternative way', 'coefficient b1', 'annual variable', 'total-factor productivity', 'reliable time series', 'large size', 'low uncertainty', 'unified measure', 'Census data', 'real uncertainty', 'time series movement', 'detailed examination', 'economic theory', 'notable difference', 'common final good', 'large negative', 'external firm', 'second moment', 'individual volatility series', 'infrequent nature', 'data coverage', 'sample selection', 'small sample', 'cross-sectional employment growth', 'time-varying measurement error', 'interquartile range', 'establishment-level employment', 'stochastic volatility', 'invertibility condition', 'standard setup', 'low resale value', 'nbloom @ stanford.edu', 'alternative driver', 'large negative technology', 'Point-wise standard', 'partial equilibrium', 'aggregate volatility measure', 'average cross-sectional variance', 'internal accounts—which', 'second way', 'broad crosscheck', 'desired effect', 'annual basis', 'population census', 'employment growth', 'establishment level', 'aggregated data', 'macro level', 'small number', 'publicly-quoted firm level', 'establishment level uncertainty', 'positive productivity shock', 'alternative uncertainty', 'standard deviation', 'establishment reallocation', 't+0.5 xi', 'rich micro-data set', 'cyclical bias', 'cyclical uncertainty', 'capacity utilization', 'modeling side', 'time series', 'firm-level stock', 'differential industry level', 'long-term downward trend', 'predominant purpose', 'semi-parametric estimation', 'general equilibrium model', 'oil price', 'own time series', 'unobserved heterogeneity', 'Compustat quarterly', 'first moment', 'ex ante uncertainty', 'large literature', 'previous period', 'total factor productivity', 'numerical solution', 'time series variation', 'survey estimation', 'standard Real-Business', 'alternative explanation', 'first few', 'time variation', 'consistent aggregation', 'common scale', 'level employment volatility', '4-digit industry', 't index establishment', 'option value', 'important aspect', 'desired start', 'territorial coverage', 'key term', 'high productivity', 'complete global', 'aggregate output', 'macro uncertainty', 'free online', 'average investment', 'growth rate', '90-10 spread', 'macro uncertainty indicator', 'Recent work', 'exact measure', 'first moment shock', 'regression analysis', 'sample period', 'efficient capital', 'single firm', 'establishment industry', 'measurement error', 'so-called animal', 'retail productivity growth', 'Empirical Exploration', 'previous paragraph', 'full contribution', 'initial condition assumption', 'ultimate parent', 'Selection bias', 'selection bias', 'graphical analysis', 'real variance', 'industry level dispersion', 'structural economic model', 'full year', 'course none', 'industry level cyclicality', 'similar manner', 'productivity growth', 'simple labor productivity', 'investment rate', 'annual frequency', 'weighted mean', 'certain plant', 'high uncertainty versus', 'generate stock market', 'strong increase', 'oil-price uncertainty', 'Evaluating Uncertainty', 'other control', 'central research', 'data series', 'above exercise', 'potential bias', 'new uncertainty', 'response rate', 'non-convex adjustment', 'industry level', 'cross-sectional industry dispersion', 'constructed annual uncertainty', 'accurate time series', 'technology investment', 'quarterly industry dispersion', 'seminal paper', 'firm level', 'following form', 'fiscal policy fall', 'possible sample size', 'Olley-Pakes approach', 'idiosyncratic volatility', 'non-recessionary line', 'sample size', 'recession share', 'common macro shock', 'careful comparison', 'substantial increase', 'output growth', 'capital stock', 'empirical evidence', 'cost-share approach', 'macro focus', 'exogenous process', 'uncertainty matter', 'industry volatility', 'perfect way', 'interested reader']\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "title = name_and_ds(712)['title']\n",
    "description = title+' '+get_abstract(712)\n",
    "\n",
    "#tokenizing words and getting rid of stop words\n",
    "descrip = word_tokenize(description)\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_descrip = []\n",
    "filtered_list = [\n",
    "    word for word in descrip if word.casefold() not in stop_words]\n",
    "#print(filtered_list)\n",
    "\n",
    "#tagging POS\n",
    "descrip_pos = nltk.pos_tag(descrip)\n",
    "\n",
    "#chunking-- phrases with a specific format\n",
    "#noun phrase that starts w/any # adjs, and ends w/any # nouns\n",
    "grammar = \"NP: {<JJ>*<NN>*}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(descrip_pos)\n",
    "phrase = []\n",
    "# stores all the phrases, key indexed by the # of words in the phrase\n",
    "chunks = {}\n",
    "for s in tree.subtrees(filter=lambda tree: tree.label() == 'NP'):\n",
    "    #print([x[0] for x in s.leaves()])\n",
    "    for x in s.leaves():\n",
    "        phrase.append(x[0])\n",
    "    #print(phrase)\n",
    "    size = len(phrase)\n",
    "    full = \"\"\n",
    "    for i in range(size):\n",
    "        if full==\"\":\n",
    "            full = phrase[i]\n",
    "        else:\n",
    "            full = full+\" \"+phrase[i]\n",
    "    if chunks.get(str(size)) is not None:\n",
    "        chunks.get(str(size)).append(full)\n",
    "    else:\n",
    "        chunks[str(size)] = [full]\n",
    "    phrase = []\n",
    "#print(\"chunks\")\n",
    "#print(chunks)           \n",
    "#tree.draw()\n",
    "\n",
    "#freq distribution\n",
    "freq_words = []\n",
    "frequency_distribution = FreqDist(filtered_list)\n",
    "#print(frequency_distribution.most_common(20))\n",
    "for i in range(20):\n",
    "    word = frequency_distribution.most_common(20)[i][0]\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    else:\n",
    "        freq_words.append(word)\n",
    "print(\"most frequent words\")\n",
    "print(freq_words)\n",
    "\n",
    "#print(collections.counter())\n",
    "\n",
    "#collocation- sequence of words that shows up often\n",
    "# print(\"collocations\")\n",
    "# print(nltk.Text(description).collocation_list())\n",
    "\n",
    "#ngrams, groups of n words\n",
    "from nltk import ngrams, FreqDist\n",
    "all_ngrams = {}\n",
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = FreqDist(ngrams(filtered_list, size))\n",
    "#print(all_counts[2].most_common(5))\n",
    "third=''\n",
    "fourth=''\n",
    "fifth=''\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        first = all_counts[i+2].most_common(5)[j][0][0]\n",
    "        second = all_counts[i+2].most_common(5)[j][0][1]\n",
    "        if i+2 == 2:\n",
    "            if len(first) == 1 or len(second) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second]\n",
    "        if i+2 == 3:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third]\n",
    "        if i+2 == 4:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth]\n",
    "        if i+2 == 5:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            fifth = all_counts[i+2].most_common(5)[j][0][4]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1 or len(fifth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth]\n",
    "                \n",
    "#print(\"ngrams\")                \n",
    "#print(all_ngrams)\n",
    "#print(all_counts[3].most_common(5))\n",
    "\n",
    "keywords = []\n",
    "if all_ngrams.get(\"2\") is not None:\n",
    "    keywords += all_ngrams.get(\"2\")\n",
    "if chunks.get(\"2\") is not None:\n",
    "    keywords += chunks.get(\"2\")\n",
    "if chunks.get(\"3\") is not None:\n",
    "    keywords += chunks.get(\"3\")\n",
    "print(\"keywords w/duplicates\")\n",
    "#print(keywords)\n",
    "print(len(keywords))\n",
    "\n",
    "print(\"final keywords\")\n",
    "kw_set = set(keywords) #caps sensitive-- same words but dif capitalization don't get removed\n",
    "keywords = list(kw_set)\n",
    "#print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_matches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "        host=\"web5.ces.census.gov\",\n",
    "        user=##,\n",
    "        password=##,\n",
    "        database=\"pmt\"\n",
    "        )\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "    \n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "#pjid = 712\n",
    "nlp_data = []\n",
    "nlp_rows = {}\n",
    "nlp_res = {}\n",
    "\n",
    "title = name_and_ds(pjid)['title']\n",
    "description = title+' '+get_abstract(pjid)\n",
    "\n",
    "'''NLP'''\n",
    "#tokenizing words and getting rid of stop words\n",
    "descrip = word_tokenize(description)\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_descrip = []\n",
    "filtered_list = [\n",
    "    word for word in descrip if word.casefold() not in stop_words]\n",
    "#print(filtered_list)\n",
    "\n",
    "#tagging POS\n",
    "descrip_pos = nltk.pos_tag(descrip)\n",
    "\n",
    "#chunking-- phrases with a specific format\n",
    "#noun phrase that starts w/any # adjs, and ends w/any # nouns\n",
    "grammar = \"NP: {<JJ>*<NN>*}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(descrip_pos)\n",
    "phrase = []\n",
    "# stores all the phrases, key indexed by the # of words in the phrase\n",
    "chunks = {}\n",
    "for s in tree.subtrees(filter=lambda tree: tree.label() == 'NP'):\n",
    "    #print([x[0] for x in s.leaves()])\n",
    "    for x in s.leaves():\n",
    "        phrase.append(x[0])\n",
    "    #print(phrase)\n",
    "    size = len(phrase)\n",
    "    full = \"\"\n",
    "    for i in range(size):\n",
    "        if full==\"\":\n",
    "            full = phrase[i]\n",
    "        else:\n",
    "            full = full+\" \"+phrase[i]\n",
    "    if chunks.get(str(size)) is not None:\n",
    "        chunks.get(str(size)).append(full)\n",
    "    else:\n",
    "        chunks[str(size)] = [full]\n",
    "    phrase = []\n",
    "#print(\"chunks\")\n",
    "#print(chunks)           \n",
    "#tree.draw()\n",
    "\n",
    "#freq distribution\n",
    "freq_words = []\n",
    "frequency_distribution = FreqDist(filtered_list)\n",
    "#print(frequency_distribution.most_common(20))\n",
    "for i in range(20):\n",
    "    word = frequency_distribution.most_common(20)[i][0]\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    else:\n",
    "        freq_words.append(word)\n",
    "print(\"most frequent words\")\n",
    "print(freq_words)\n",
    "\n",
    "#print(collections.counter())\n",
    "\n",
    "#collocation- sequence of words that shows up often\n",
    "# print(\"collocations\")\n",
    "# print(nltk.Text(description).collocation_list())\n",
    "\n",
    "#ngrams, groups of n words\n",
    "all_ngrams = {}\n",
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = FreqDist(ngrams(filtered_list, size))\n",
    "#print(all_counts[2].most_common(5))\n",
    "third=''\n",
    "fourth=''\n",
    "fifth=''\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        first = all_counts[i+2].most_common(5)[j][0][0]\n",
    "        second = all_counts[i+2].most_common(5)[j][0][1]\n",
    "        if i+2 == 2:\n",
    "            if len(first) == 1 or len(second) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second]\n",
    "        if i+2 == 3:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third]\n",
    "        if i+2 == 4:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth]\n",
    "        if i+2 == 5:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            fifth = all_counts[i+2].most_common(5)[j][0][4]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1 or len(fifth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth]\n",
    "\n",
    "#print(\"ngrams\")                \n",
    "#print(all_ngrams)\n",
    "#print(all_counts[3].most_common(5))\n",
    "\n",
    "keywords = []\n",
    "if all_ngrams.get(\"2\") is not None:\n",
    "    keywords += all_ngrams.get(\"2\")\n",
    "if chunks.get(\"2\") is not None:\n",
    "    keywords += chunks.get(\"2\")\n",
    "if chunks.get(\"3\") is not None:\n",
    "    keywords += chunks.get(\"3\")\n",
    "print(\"keywords w/duplicates\")\n",
    "#print(keywords)\n",
    "print(len(keywords))\n",
    "\n",
    "print(\"final keywords\")\n",
    "kw_set = set(keywords) #caps sensitive-- same words but dif capitalization don't get removed\n",
    "keywords = list(kw_set)\n",
    "#print(keywords)\n",
    "print(len(keywords))\n",
    "\n",
    "'''title keywords in description'''\n",
    "q3 = \"SELECT * FROM api_base WHERE proj_id= %s AND (description LIKE %s\"\n",
    "d3 = (pjid, keywords[0])\n",
    "l3 = list(d3)\n",
    "numData = len(keywords)\n",
    "for i in range(numData-1):\n",
    "    q3 += \" OR description LIKE %s\"\n",
    "    if i == numData-2:\n",
    "        q3 += \")\"\n",
    "    l3.append(str('%' + keywords[i+1] + '%'))\n",
    "d3 = tuple(l3)\n",
    "mycursor.execute(q3, d3)\n",
    "r3 = mycursor.fetchall()\n",
    "nlp_data.append(len(r3))\n",
    "for i in range(len(r3)):\n",
    "    if nlp_rows.get(r3[i][0]) is not None:\n",
    "        nlp_rows[r3[i][0]] = nlp_rows.get(r3[i][0])+1\n",
    "    else:\n",
    "        nlp_rows[r3[i][0]] = 1\n",
    "\n",
    "\n",
    "'''title keywords in title'''\n",
    "q4 = \"SELECT * FROM api_base WHERE proj_id= %s AND (title LIKE %s\"\n",
    "d4 = (pjid, keywords[0])\n",
    "l4 = list(d4)\n",
    "numData = len(keywords)\n",
    "for i in range(numData-1):\n",
    "    q4 += \" OR title LIKE %s\"\n",
    "    if i == numData-2:\n",
    "        q4 += \")\"\n",
    "    l4.append(str('%' + keywords[i+1] + '%'))\n",
    "d4 = tuple(l4)\n",
    "mycursor.execute(q4, d4)\n",
    "r4 = mycursor.fetchall()\n",
    "nlp_data.append(len(r4))\n",
    "for i in range(len(r4)):\n",
    "    if nlp_rows.get(r4[i][0]) is not None:\n",
    "        nlp_rows[r4[i][0]] = nlp_rows.get(r4[i][0])+1\n",
    "    else:\n",
    "        nlp_rows[r4[i][0]] = 1\n",
    "\n",
    "descrips = {}\n",
    "for key in nlp_rows.keys():\n",
    "    #print(type(key))\n",
    "    q = \"SELECT title FROM api_base WHERE pub_id = \" +str(key)\n",
    "    mycursor.execute(q)\n",
    "    r = mycursor.fetchall()\n",
    "    descrips[key] = [r[0][0]]\n",
    "    qq = \"SELECT description FROM api_base WHERE pub_id = \" +str(key)\n",
    "    mycursor.execute(qq)\n",
    "    rr = mycursor.fetchall()\n",
    "    descrips.get(key).append(rr[0][0])\n",
    "\n",
    "#print(descrips)\n",
    "print(nlp_rows)\n",
    "\n",
    "for i in range(len(keywords)):\n",
    "    for key in descrips.keys():\n",
    "        if keywords[i] in descrips[key][0]:\n",
    "            nlp_rows[key] = nlp_rows.get(key)+1\n",
    "        if keywords[i] in descrips[key][1]:\n",
    "            nlp_rows[key] = nlp_rows.get(key)+1\n",
    "\n",
    "print(nlp_rows)\n",
    "nlp_matches[pjid] = nlp_data\n",
    "print(nlp_matches)\n",
    "#nlp_res[pjid] = nlp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'FreqDist' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-411e47e16eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch_NLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-afbdfa0f2bea>\u001b[0m in \u001b[0;36msearch_NLP\u001b[1;34m(pjid)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;31m#freq distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mfreq_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mfrequency_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;31m#print(frequency_distribution.most_common(20))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'FreqDist' referenced before assignment"
     ]
    }
   ],
   "source": [
    "search_NLP(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-7f8dd0e2df65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'matches' is not defined"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foreign trade', 'Foreign Trade']\n"
     ]
    }
   ],
   "source": [
    "test1 = [\"foreign trade\", \"Foreign Trade\"] \n",
    "test2 = set(test1)\n",
    "print(list(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cms authors'''\n",
    "import base64\n",
    "from base64 import b64encode\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "api_key = ##\n",
    "\n",
    "headers = { 'Authorization' : 'key=%s' %  (api_key).decode(\"ascii\") }\n",
    "\n",
    "def run_query(path,fields,page,limit):\n",
    "    request = requests.get('https://www.ces.census.gov/api/iaa/v1.3%s?fields=%s&page=%s&limit=%s' % (path,fields,page,limit), headers=headers, proxies={\"https\" : \"148.129.75.18:3128\"})\n",
    "    if request.status_code == 200:\n",
    "        return request.json()\n",
    "    else:\n",
    "        raise Exception(\"Query failed to run by returning code of {}.\".format(request.status_code))\n",
    "\n",
    "def get_authors(pjid):\n",
    "    projects = run_query('/project/'+str(pjid),'cms_project_id,type_name,title,participants{username,fullname,roles}','0','100') # Execute the query\n",
    "\n",
    "    auts = []\n",
    "    \n",
    "    for proj in projects:\n",
    "        #print(proj['title'])\n",
    "        if proj['title'].split(\"-\")[0] == str(pjid) + \" \":\n",
    "            for part in proj['participants']:\n",
    "                if 'researcher' in part['roles']:\n",
    "                    auts.append(part['fullname'])\n",
    "                    \n",
    "    return auts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J Bradford Jensen', 'Andrew B Bernard', 'Peter K Schott']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_authors(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "title = name_and_ds(712)['title']\n",
    "description = title+' '+get_abstract(712)\n",
    "\n",
    "#tokenizing words and getting rid of stop words\n",
    "descrip = word_tokenize(description)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_descrip = []\n",
    "filtered_list = [\n",
    "    word for word in descrip if word.casefold() not in stop_words]\n",
    "#print(filtered_list)\n",
    "\n",
    "#tagging POS\n",
    "descrip_pos = nltk.pos_tag(descrip)\n",
    "\n",
    "#chunking-- phrases with a specific format\n",
    "#noun phrase that starts w/any # adjs, and ends w/any # nouns\n",
    "grammar = \"NP: {<JJ>*<NN>*}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(descrip_pos)\n",
    "phrase = []\n",
    "# stores all the phrases, key indexed by the # of words in the phrase\n",
    "chunks = {}\n",
    "for s in tree.subtrees(filter=lambda tree: tree.label() == 'NP'):\n",
    "    #print([x[0] for x in s.leaves()])\n",
    "    for x in s.leaves():\n",
    "        phrase.append(x[0])\n",
    "    #print(phrase)\n",
    "    size = len(phrase)\n",
    "    full = \"\"\n",
    "    for i in range(size):\n",
    "        if full==\"\":\n",
    "            full = phrase[i]\n",
    "        else:\n",
    "            full = full+\" \"+phrase[i]\n",
    "    if chunks.get(str(size)) is not None:\n",
    "        chunks.get(str(size)).append(full)\n",
    "    else:\n",
    "        chunks[str(size)] = [full]\n",
    "    phrase = []\n",
    "# print(\"chunks\")\n",
    "# print(chunks)           \n",
    "#tree.draw()\n",
    "\n",
    "#freq distribution\n",
    "freq_words = []\n",
    "frequency_distribution = FreqDist(filtered_list)\n",
    "#print(frequency_distribution.most_common(20))\n",
    "for i in range(20):\n",
    "    word = frequency_distribution.most_common(20)[i][0]\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    else:\n",
    "        freq_words.append(word)\n",
    "# print(\"most frequent words\")\n",
    "# print(freq_words)\n",
    "\n",
    "#print(collections.counter())\n",
    "\n",
    "#collocation- sequence of words that shows up often\n",
    "# print(\"collocations\")\n",
    "# print(nltk.Text(description).collocation_list())\n",
    "\n",
    "#ngrams, groups of n words\n",
    "from nltk import ngrams, FreqDist\n",
    "all_ngrams = {}\n",
    "all_counts = dict()\n",
    "for size in 2, 3, 4, 5:\n",
    "    all_counts[size] = FreqDist(ngrams(filtered_list, size))\n",
    "#print(all_counts[2].most_common(5))\n",
    "third=''\n",
    "fourth=''\n",
    "fifth=''\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        first = all_counts[i+2].most_common(5)[j][0][0]\n",
    "        second = all_counts[i+2].most_common(5)[j][0][1]\n",
    "        if i+2 == 2:\n",
    "            if len(first) == 1 or len(second) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second]\n",
    "        if i+2 == 3:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third]\n",
    "        if i+2 == 4:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth]\n",
    "        if i+2 == 5:\n",
    "            third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "            fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "            fifth = all_counts[i+2].most_common(5)[j][0][4]\n",
    "            if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1 or len(fifth) == 1:\n",
    "                continue\n",
    "            if all_ngrams.get(str(i+2)) is not None:\n",
    "                all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth)\n",
    "            else:\n",
    "                all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth]\n",
    "                \n",
    "#print(\"ngrams\")                \n",
    "#print(all_ngrams)\n",
    "#print(all_counts[3].most_common(5))\n",
    "\n",
    "keywords = []\n",
    "if all_ngrams.get(\"2\") is not None:\n",
    "    keywords += all_ngrams.get(\"2\")\n",
    "if chunks.get(\"2\") is not None:\n",
    "    keywords += chunks.get(\"2\")\n",
    "if chunks.get(\"3\") is not None:\n",
    "    keywords += chunks.get(\"3\")\n",
    "#print(\"keywords w/duplicates\")\n",
    "#print(keywords)\n",
    "#print(len(keywords))\n",
    "\n",
    "#print(\"final keywords\")\n",
    "kw_set = set(keywords) #caps sensitive-- same words but dif capitalization don't get removed\n",
    "keywords = list(kw_set)\n",
    "#print(keywords)\n",
    "#print(len(keywords))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_NLP(pjid, year):\n",
    "    \n",
    "    mydb = mysql.connector.connect(\n",
    "        host=\"web5.ces.census.gov\",\n",
    "        user=##,\n",
    "        password=##,\n",
    "        database=\"pmt\"\n",
    "        )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    \n",
    "    import nltk\n",
    "    #nltk.set_proxy('http://proxy.tco.census.gov:3128')\n",
    "    #nltk.download(\"punkt\")\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk import FreqDist\n",
    "    from nltk import ngrams\n",
    "\n",
    "    '''NLP'''\n",
    "    title = name_and_ds(pjid)['title']\n",
    "    description = title+' '+get_abstract(pjid)\n",
    "\n",
    "    #tokenizing words and getting rid of stop words\n",
    "    descrip = word_tokenize(description)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_descrip = []\n",
    "    filtered_list = [\n",
    "        word for word in descrip if word.casefold() not in stop_words]\n",
    "    #print(filtered_list)\n",
    "\n",
    "    #tagging POS\n",
    "    descrip_pos = nltk.pos_tag(descrip)\n",
    "\n",
    "    #chunking-- phrases with a specific format\n",
    "    #noun phrase that starts w/any # adjs, and ends w/any # nouns\n",
    "    grammar = \"NP: {<JJ>*<NN>*}\"\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(descrip_pos)\n",
    "    phrase = []\n",
    "    # stores all the phrases, key indexed by the # of words in the phrase\n",
    "    chunks = {}\n",
    "    for s in tree.subtrees(filter=lambda tree: tree.label() == 'NP'):\n",
    "        #print([x[0] for x in s.leaves()])\n",
    "        for x in s.leaves():\n",
    "            phrase.append(x[0])\n",
    "        #print(phrase)\n",
    "        size = len(phrase)\n",
    "        full = \"\"\n",
    "        for i in range(size):\n",
    "            if full==\"\":\n",
    "                full = phrase[i]\n",
    "            else:\n",
    "                full = full+\" \"+phrase[i]\n",
    "        if chunks.get(str(size)) is not None:\n",
    "            chunks.get(str(size)).append(full)\n",
    "        else:\n",
    "            chunks[str(size)] = [full]\n",
    "        phrase = []\n",
    "    # print(\"chunks\")\n",
    "    # print(chunks)           \n",
    "    #tree.draw()\n",
    "\n",
    "    #freq distribution\n",
    "    freq_words = []\n",
    "    frequency_distribution = FreqDist(filtered_list)\n",
    "    #print(frequency_distribution.most_common(20))\n",
    "    for i in range(20):\n",
    "        word = frequency_distribution.most_common(20)[i][0]\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            freq_words.append(word)\n",
    "    # print(\"most frequent words\")\n",
    "    # print(freq_words)\n",
    "\n",
    "    #print(collections.counter())\n",
    "\n",
    "    #collocation- sequence of words that shows up often\n",
    "    # print(\"collocations\")\n",
    "    # print(nltk.Text(description).collocation_list())\n",
    "\n",
    "    #ngrams, groups of n words\n",
    "    from nltk import ngrams, FreqDist\n",
    "    all_ngrams = {}\n",
    "    all_counts = dict()\n",
    "    for size in 2, 3, 4, 5:\n",
    "        all_counts[size] = FreqDist(ngrams(filtered_list, size))\n",
    "    #print(all_counts[2].most_common(5))\n",
    "    third=''\n",
    "    fourth=''\n",
    "    fifth=''\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            first = all_counts[i+2].most_common(5)[j][0][0]\n",
    "            second = all_counts[i+2].most_common(5)[j][0][1]\n",
    "            if i+2 == 2:\n",
    "                if len(first) == 1 or len(second) == 1:\n",
    "                    continue\n",
    "                if all_ngrams.get(str(i+2)) is not None:\n",
    "                    all_ngrams.get(str(i+2)).append(first + \" \" + second)\n",
    "                else:\n",
    "                    all_ngrams[str(i+2)] = [first + \" \" + second]\n",
    "            if i+2 == 3:\n",
    "                third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "                if len(first) == 1 or len(second) == 1 or len(third) == 1:\n",
    "                    continue\n",
    "                if all_ngrams.get(str(i+2)) is not None:\n",
    "                    all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third)\n",
    "                else:\n",
    "                    all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third]\n",
    "            if i+2 == 4:\n",
    "                third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "                fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "                if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1:\n",
    "                    continue\n",
    "                if all_ngrams.get(str(i+2)) is not None:\n",
    "                    all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth)\n",
    "                else:\n",
    "                    all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth]\n",
    "            if i+2 == 5:\n",
    "                third = all_counts[i+2].most_common(5)[j][0][2]\n",
    "                fourth = all_counts[i+2].most_common(5)[j][0][3]\n",
    "                fifth = all_counts[i+2].most_common(5)[j][0][4]\n",
    "                if len(first) == 1 or len(second) == 1 or len(third) == 1 or len(fourth) == 1 or len(fifth) == 1:\n",
    "                    continue\n",
    "                if all_ngrams.get(str(i+2)) is not None:\n",
    "                    all_ngrams.get(str(i+2)).append(first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth)\n",
    "                else:\n",
    "                    all_ngrams[str(i+2)] = [first + \" \" + second + \" \" + third + \" \" + fourth + \" \" + fifth]\n",
    "\n",
    "    #print(\"ngrams\")                \n",
    "    #print(all_ngrams)\n",
    "    #print(all_counts[3].most_common(5))\n",
    "\n",
    "    keywords = []\n",
    "    if all_ngrams.get(\"2\") is not None:\n",
    "        keywords += all_ngrams.get(\"2\")\n",
    "    if chunks.get(\"2\") is not None:\n",
    "        keywords += chunks.get(\"2\")\n",
    "    if chunks.get(\"3\") is not None:\n",
    "        keywords += chunks.get(\"3\")\n",
    "    #print(\"keywords w/duplicates\")\n",
    "    #print(keywords)\n",
    "    #print(len(keywords))\n",
    "\n",
    "    #print(\"final keywords\")\n",
    "    kw_set = set(keywords) #caps sensitive-- same words but dif capitalization don't get removed\n",
    "    keywords = list(kw_set)\n",
    "    #print(keywords)\n",
    "    #print(len(keywords))\n",
    "    print(\"done NLP\")\n",
    "\n",
    "    '''end NLP'''\n",
    "\n",
    "    #description = \"The project will proceed in three phases. The first will enhance the export and import information on Economic Censuses and Surveys. We will develop and test linkages between transaction level Foreign Trade data and Economic Census and Survey data and compare the links developed by CES to those developed by the Census Bureau’s Foreign Trade Division (FTD). If improvements in linkages methods are identified, we will provide advice to FTD. The second phase will analyze transaction level detail to examine changes in foreign trade transactions, focusing initially on related party transactions, potential underreporting, and impact of FTD outreach efforts. This component of the project will focus on identifying reasons for such changes. The third phase of the project will develop empirical and analytical framework to investigate the impact of trade on the U.S. economy - focusing on how firms allocate economic activity between domestic and foreign production and the impact of this on the domestic economy (including workers and regional economies). This component makes use of the data developed in the previous phases to develop new estimates of the impact of foreign trade on U.S. industries. It will examine how imports and exports affect domestic production, employment, and productivity. It will also examine how firm responses to trade affect local labor market outcomes.\"\n",
    "    # key is author's name, values are the queries for that name\n",
    "    # for each author name present, get an additional point\n",
    "\n",
    "    '''inputs'''\n",
    "    #pjid = 712 #given\n",
    "    #year = 2009 #given\n",
    "\n",
    "    res = {} # of points per pub_id for each pjid\n",
    "    matches = {} # of hits for each query for each pjid\n",
    "    authors = {}\n",
    "\n",
    "    auts = get_authors(pjid) #array of authors' names\n",
    "    numAuts = len(auts)\n",
    "    title = name_and_ds(pjid)['title']\n",
    "    # for each author, split into two search queries-- one w/ and one w/out middle initial\n",
    "\n",
    "    for i in range(numAuts):\n",
    "        part = auts[i].partition(' ')\n",
    "        aut1 = auts[i]\n",
    "        aut2 = part[0] + ' ' + part[2].partition(' ')[2]\n",
    "        first = part[0]\n",
    "        middle = part[2].partition(' ')[0]\n",
    "        last = part[2].partition(' ')[2]\n",
    "        #print(part[2].partition(' ')[2])\n",
    "        if len(part[0]) == 1:\n",
    "            first = part[0]+\".\"\n",
    "        if len(part[2].partition(' ')[0]) == 1:\n",
    "            middle = part[2].partition(' ')[0] + \".\"\n",
    "        if len(part[2].partition(' ')[2]) == 1:\n",
    "            last = part[2].partition(' ')[2] + \".\"\n",
    "        aut3 = first + \" \" + last\n",
    "        aut4 = first + \" \" + middle + \" \" + last\n",
    "        authors[i] = [aut1, aut2, aut3, aut4]\n",
    "\n",
    "    ds = name_and_ds(pjid)['datasets'] #array\n",
    "\n",
    "    params_inner = {}\n",
    "    params = {}\n",
    "\n",
    "    params_inner['year'] = year\n",
    "    params_inner['authors'] = auts\n",
    "    params_inner['datasets'] = ds\n",
    "    params_inner['keywords'] = keywords\n",
    "    params[pjid] = params_inner\n",
    "\n",
    "    #array of how many hits each query gets\n",
    "    data = []\n",
    "\n",
    "    '''pub_year'''\n",
    "    q1 = \"SELECT * FROM api_base WHERE proj_id= %s AND (pub_year >= %s OR pub_year is NULL)\"\n",
    "    d1 = (pjid, year)\n",
    "    mycursor.execute(q1, d1)\n",
    "    r1 = mycursor.fetchall()\n",
    "    data.append(len(r1))\n",
    "    rows = {}\n",
    "    for i in range(len(r1)):\n",
    "        if rows.get(r1[i][0]) is not None:\n",
    "            rows[r1[i][0]] = rows.get(r1[i][0])+1\n",
    "        else:\n",
    "            rows[r1[i][0]] = 1\n",
    "    #print(r1)\n",
    "    #print(type(rows))\n",
    "    #print(rows)\n",
    "\n",
    "    '''authors'''\n",
    "    aut_hits = 0\n",
    "\n",
    "    def refineAut(pjid, aut1, aut2, aut3, aut4):\n",
    "        q2 = \"SELECT * FROM api_base_participants WHERE proj_id= %s AND (fullname = %s OR fullname = %s OR fullname = %s OR fullname = %s)\"\n",
    "        d2 = (pjid, aut1, aut2, aut3, aut4)\n",
    "        mycursor.execute(q2, d2)\n",
    "        r2 = mycursor.fetchall()\n",
    "        #aut_hits += len(r2)\n",
    "        for i in range(len(r2)):\n",
    "            if rows.get(r2[i][0]) is not None:\n",
    "                rows[r2[i][0]] = rows.get(r2[i][0])+1\n",
    "        return len(r2)\n",
    "\n",
    "    for i in range(numAuts):\n",
    "        aut_hits += refineAut(pjid, authors.get(i)[0], authors.get(i)[1], authors.get(i)[2], authors.get(i)[3])\n",
    "        #print(authors.get(i))\n",
    "        #print(aut_hits)\n",
    "\n",
    "    data.append(aut_hits)\n",
    "\n",
    "    nlp_matches = {}\n",
    "    nlp_data = []\n",
    "    nlp_rows = {}\n",
    "    nlp_res = {}\n",
    "\n",
    "    '''queries 3 and 4: one pt each for having at least one matching word, 1 add. pt for each matching word'''\n",
    "    '''title keywords in description'''\n",
    "    q3 = \"SELECT * FROM api_base WHERE proj_id= %s AND (description LIKE %s\"\n",
    "    d3 = (pjid, keywords[0])\n",
    "    l3 = list(d3)\n",
    "    numData = len(keywords)\n",
    "    for i in range(numData-1):\n",
    "        q3 += \" OR description LIKE %s\"\n",
    "        if i == numData-2:\n",
    "            q3 += \")\"\n",
    "        l3.append(str('%' + keywords[i+1] + '%'))\n",
    "    d3 = tuple(l3)\n",
    "    mycursor.execute(q3, d3)\n",
    "    r3 = mycursor.fetchall()\n",
    "    nlp_data.append(len(r3)) #amount of hits that have at least one matching word\n",
    "    for i in range(len(r3)):\n",
    "        if nlp_rows.get(r3[i][0]) is not None:\n",
    "            nlp_rows[r3[i][0]] = nlp_rows.get(r3[i][0])+1\n",
    "        else:\n",
    "            nlp_rows[r3[i][0]] = 1\n",
    "\n",
    "    '''title keywords in title'''\n",
    "    q4 = \"SELECT * FROM api_base WHERE proj_id= %s AND (title LIKE %s\"\n",
    "    d4 = (pjid, keywords[0])\n",
    "    l4 = list(d4)\n",
    "    numData = len(keywords)\n",
    "    for i in range(numData-1):\n",
    "        q4 += \" OR title LIKE %s\"\n",
    "        if i == numData-2:\n",
    "            q4 += \")\"\n",
    "        l4.append(str('%' + keywords[i+1] + '%'))\n",
    "    d4 = tuple(l4)\n",
    "    mycursor.execute(q4, d4)\n",
    "    r4 = mycursor.fetchall()\n",
    "    nlp_data.append(len(r4)) #amount of hits with at least one matching word\n",
    "    for i in range(len(r4)):\n",
    "        if nlp_rows.get(r4[i][0]) is not None:\n",
    "            nlp_rows[r4[i][0]] = nlp_rows.get(r4[i][0])+1\n",
    "        else:\n",
    "            nlp_rows[r4[i][0]] = 1\n",
    "\n",
    "    descrips = {}\n",
    "    #grab and store titles and descriptions of all publications that had at least one match in title or descrip\n",
    "    for key in nlp_rows.keys():\n",
    "        #print(type(key))\n",
    "        q = \"SELECT title FROM api_base WHERE pub_id = \" +str(key)\n",
    "        mycursor.execute(q)\n",
    "        r = mycursor.fetchall()\n",
    "        descrips[key] = [r[0][0]]\n",
    "        qq = \"SELECT description FROM api_base WHERE pub_id = \" +str(key)\n",
    "        mycursor.execute(qq)\n",
    "        rr = mycursor.fetchall()\n",
    "        descrips.get(key).append(rr[0][0])\n",
    "\n",
    "    #print(descrips)\n",
    "    #print(nlp_rows)\n",
    "\n",
    "    title_hits = 0\n",
    "    descrip_hits = 0\n",
    "\n",
    "    #print(keywords[0])\n",
    "    #if descrips.keys() is not None:\n",
    "    for i in range(len(keywords)):\n",
    "        for key in descrips.keys():\n",
    "            #print(key)\n",
    "            #(keywords[i])\n",
    "            #print(descrips[key][0])\n",
    "            #print(key)\n",
    "            #print(nlp_rows)\n",
    "            if descrips[key][0] is not None:\n",
    "                if keywords[i] in descrips[key][0]:\n",
    "                    if nlp_rows[key] is not None:\n",
    "                        nlp_rows[key] = nlp_rows.get(key)+1\n",
    "                        title_hits += 1\n",
    "            if descrips[key][1] is not None:\n",
    "                if keywords[i] in descrips[key][1]:\n",
    "                    if nlp_rows[key] is not None:\n",
    "                        nlp_rows[key] = nlp_rows.get(key)+1\n",
    "                        descrip_hits += 1\n",
    "\n",
    "    data.append(title_hits)\n",
    "    data.append(descrip_hits)\n",
    "\n",
    "    #print(nlp_rows)\n",
    "    nlp_matches[pjid] = nlp_data\n",
    "    #print(nlp_matches)\n",
    "    nlp_res[pjid] = nlp_rows\n",
    "\n",
    "\n",
    "    '''datasets in description'''\n",
    "    q5 = \"SELECT * FROM api_base WHERE proj_id= %s AND (description LIKE %s\"\n",
    "    d5 = (pjid, ds[0])\n",
    "    l5 = list(d5)\n",
    "    numData = len(ds)\n",
    "    for i in range(numData-1):\n",
    "        q5 += \" OR description LIKE %s\"\n",
    "        if i == numData-2:\n",
    "            q5 += \")\"\n",
    "        l5.append(str('%' + ds[i+1] + '%'))\n",
    "    d5 = tuple(l5)\n",
    "    #print(q5)\n",
    "    #print(d5)\n",
    "    mycursor.execute(q5, d5)\n",
    "    r5 = mycursor.fetchall()\n",
    "    data.append(len(r5))\n",
    "    for i in range(len(r5)):\n",
    "        if rows.get(r5[i][0]) is not None:\n",
    "            rows[r5[i][0]] = rows.get(r5[i][0])+1\n",
    "        #else:\n",
    "            #rows[r5[i][0]] = 1\n",
    "    #print(\"r5:\")\n",
    "    #print(r5)\n",
    "    #print(rows)\n",
    "\n",
    "    for key in rows.keys():\n",
    "        if nlp_rows.get(key) is not None:\n",
    "            rows[key] = rows.get(key)+nlp_rows.get(key)\n",
    "    #print(rows)\n",
    "\n",
    "    res_sorted = {}\n",
    "\n",
    "    for w in sorted(rows, key = rows.get, \\\n",
    "                    reverse = True):\n",
    "        res_sorted[w] = rows[w]\n",
    "        #print(w, rows[w])\n",
    "\n",
    "    print(\"sorted results:\")\n",
    "    print(res_sorted)\n",
    "    #print(rows)\n",
    "    res[pjid] = rows\n",
    "    matches[pjid] = data\n",
    "    #print(res)\n",
    "    print(\"matches:\")\n",
    "    print(matches)\n",
    "    #print(\"params:\")\n",
    "    #print(params)\n",
    "    #print(\"total hits: \" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew B. Bernard\n"
     ]
    }
   ],
   "source": [
    "auts = get_authors(5)\n",
    "part = auts[1].partition(' ')\n",
    "first = part[0]\n",
    "middle = part[2].partition(' ')[0]\n",
    "last = part[2].partition(' ')[2]\n",
    "#print(part[2].partition(' ')[2])\n",
    "if len(part[0]) == 1:\n",
    "    first = part[0]+\".\"\n",
    "if len(part[2].partition(' ')[0]) == 1:\n",
    "    middle = part[2].partition(' ')[0] + \".\"\n",
    "if len(part[2].partition(' ')[2]) == 1:\n",
    "    last = part[2].partition(' ')[2] + \".\"\n",
    "print(first + \" \" + middle + \" \" + last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done NLP\n",
      "sorted results:\n",
      "{573: 9, 555: 7, 561: 7, 570: 7, 571: 7, 586: 7, 597: 7, 623: 7, 630: 7, 635: 7, 589: 6, 542: 5, 552: 5, 554: 5, 556: 5, 565: 5, 580: 5, 585: 5, 601: 5, 644: 5, 553: 4, 582: 4, 596: 4, 604: 4, 619: 4, 638: 4, 546: 3, 569: 3, 579: 3, 602: 3, 605: 3, 631: 3, 632: 3, 636: 3, 643: 3, 544: 2, 545: 2, 549: 2, 558: 2, 562: 2, 563: 2, 564: 2, 566: 2, 567: 2, 568: 2, 574: 2, 581: 2, 587: 2, 588: 2, 590: 2, 592: 2, 593: 2, 595: 2, 603: 2, 618: 2, 626: 2, 629: 2, 550: 1, 612: 1, 624: 1, 634: 1}\n",
      "matches:\n",
      "{1284: [61, 99, 1, 61, 0]}\n"
     ]
    }
   ],
   "source": [
    "search_NLP(1284, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res:\n",
      "{5: {1: 2, 2: 2, 3: 2, 4: 1, 5: 2, 6: 2, 7: 2, 8: 2, 9: 2, 10: 2, 11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2, 17: 1, 19: 1, 21: 2, 22: 2, 23: 2, 24: 2, 25: 2, 26: 2, 27: 2, 28: 2, 29: 2, 30: 2, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 2, 37: 2, 38: 2, 39: 2, 40: 2, 41: 2, 42: 2, 43: 2, 44: 2, 45: 2, 46: 2, 47: 2, 48: 2, 49: 2, 50: 2, 51: 2, 52: 2, 53: 1, 54: 2, 55: 2, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2, 61: 2, 62: 2, 63: 2, 64: 2, 65: 2, 66: 2, 67: 2, 69: 2, 70: 2, 71: 2, 72: 2, 73: 2, 74: 2, 75: 2, 76: 2, 77: 2, 78: 2, 79: 2, 80: 2, 81: 2, 82: 2, 83: 2, 84: 2, 85: 1, 86: 2, 87: 2, 88: 3, 89: 2, 90: 2, 91: 2, 92: 2, 93: 1, 94: 2, 95: 2, 96: 2, 97: 2, 98: 2, 99: 2}, 343: {100: 2, 101: 2, 102: 2, 103: 2, 104: 1, 105: 2, 106: 2, 107: 2, 108: 2, 109: 2, 110: 3, 111: 2, 113: 1, 114: 1, 115: 1, 116: 2, 117: 2, 118: 2, 119: 1, 120: 2, 121: 3, 122: 2, 123: 2, 124: 2, 125: 2, 126: 2, 127: 2, 128: 1, 129: 1, 131: 1}, 580: {132: 2, 133: 2, 134: 2, 135: 2, 136: 2, 137: 2, 138: 2, 139: 2, 140: 2, 141: 2, 142: 2, 143: 1, 144: 1, 145: 2, 146: 2, 147: 2, 148: 2, 149: 2, 150: 2, 151: 3, 152: 2, 153: 2, 154: 3, 155: 3, 156: 2, 157: 2, 158: 2, 159: 2, 160: 2, 161: 3, 162: 1, 163: 3, 164: 2, 166: 2, 167: 2, 168: 2, 169: 2, 170: 2, 171: 2, 172: 2, 173: 3, 175: 1, 176: 2, 177: 3, 178: 2, 179: 2}, 861: {180: 2, 181: 3, 182: 3, 183: 2, 184: 1, 185: 3, 186: 2}, 932: {210: 1, 213: 1, 214: 1, 215: 1, 217: 1, 219: 1, 221: 1, 223: 1, 224: 1, 228: 1, 229: 1, 231: 1, 232: 1, 233: 1, 234: 1, 235: 1, 237: 1, 238: 1, 241: 1, 243: 1, 249: 1, 250: 1, 255: 1, 257: 1, 261: 1, 263: 1, 267: 1, 272: 1, 273: 1, 276: 1, 281: 1, 282: 1, 283: 1, 284: 1, 285: 1, 286: 1, 287: 1, 288: 1, 289: 1, 290: 1, 291: 1, 292: 1, 293: 1, 294: 1, 295: 1, 296: 1, 299: 1, 301: 1, 303: 1, 304: 1, 308: 1, 310: 1, 311: 1}, 712: {312: 3, 313: 2, 314: 2, 315: 2, 316: 4, 317: 2, 318: 2, 319: 2, 320: 1, 321: 1, 322: 3, 323: 2, 324: 2, 325: 2, 326: 2, 327: 2, 328: 2, 329: 2, 331: 2, 332: 2, 334: 2, 336: 2, 337: 2, 338: 1, 339: 2, 340: 5, 341: 2, 342: 2, 343: 2, 344: 4, 345: 2, 346: 5, 347: 4, 348: 2, 349: 2, 350: 2, 351: 2, 352: 5, 353: 2, 354: 2, 355: 3, 357: 2, 358: 2, 359: 2, 361: 4, 362: 2, 363: 5, 364: 2, 365: 5, 366: 2, 368: 2, 370: 2, 373: 2, 374: 4, 375: 2, 376: 2, 377: 2, 378: 2, 379: 3, 380: 2, 381: 2, 382: 2, 383: 2, 384: 3, 385: 4, 386: 1, 387: 5, 388: 2, 389: 2, 390: 5, 391: 2, 392: 2, 393: 2, 394: 2, 395: 2, 396: 2, 397: 3, 398: 2, 399: 2, 400: 5, 401: 4, 402: 5, 403: 2, 404: 2, 405: 5}, 1195: {406: 2, 407: 2, 408: 2, 409: 2, 411: 3, 413: 2, 414: 3, 415: 3, 416: 4, 417: 4, 418: 1, 419: 1, 421: 2, 422: 1, 423: 2, 424: 4, 426: 2, 427: 2, 428: 2, 429: 2, 430: 2, 431: 2, 432: 2, 434: 2, 435: 4, 436: 2, 439: 1, 441: 4, 442: 2, 443: 1, 445: 2, 448: 1}, 1277: {450: 1, 451: 1, 452: 1, 458: 1, 459: 1, 461: 1, 470: 1, 471: 1, 478: 1, 479: 1, 484: 1, 485: 1, 487: 1, 488: 1, 489: 1, 498: 1, 499: 1, 503: 1, 506: 1, 510: 1, 511: 1, 513: 1, 514: 4, 521: 1, 522: 1, 524: 1, 525: 1, 526: 5, 527: 1, 530: 1, 531: 1, 532: 2, 533: 1, 534: 1, 538: 1}, 1284: {542: 2, 544: 2, 545: 2, 546: 2, 549: 2, 550: 1, 552: 2, 553: 2, 554: 2, 555: 2, 556: 2, 558: 2, 561: 2, 562: 2, 563: 2, 564: 3, 565: 2, 566: 3, 567: 2, 568: 2, 569: 2, 570: 2, 571: 2, 573: 2, 574: 1, 579: 2, 580: 2, 581: 2, 582: 2, 585: 2, 586: 2, 587: 3, 588: 2, 589: 2, 590: 3, 592: 2, 593: 2, 595: 2, 596: 2, 597: 2, 601: 3, 602: 2, 603: 2, 604: 2, 605: 2, 612: 1, 618: 1, 619: 2, 623: 2, 624: 2, 626: 2, 629: 1, 630: 2, 631: 2, 632: 2, 634: 2, 635: 2, 636: 1, 638: 1, 643: 2, 644: 2}, 1360: {646: 1, 647: 1, 648: 1, 649: 1, 650: 1, 651: 1, 652: 1, 653: 1, 654: 1, 655: 1, 656: 1, 657: 1, 658: 1, 659: 1, 660: 1, 661: 1, 662: 1, 665: 1, 667: 1, 668: 1, 669: 1, 670: 1, 671: 1, 672: 1, 673: 1, 676: 1, 677: 1, 678: 1, 679: 1, 680: 1, 681: 1, 683: 1, 684: 1, 685: 1, 688: 1, 691: 1, 692: 2, 702: 1, 703: 1, 704: 1, 705: 1, 707: 3, 708: 1, 709: 1, 714: 1, 715: 1, 716: 1, 717: 1, 724: 1, 732: 1, 741: 1, 743: 1, 748: 1, 749: 1, 751: 1, 754: 1, 759: 1, 761: 1, 768: 1, 769: 1, 771: 1, 772: 1, 773: 1, 775: 1, 776: 1, 780: 1, 781: 1, 783: 1, 785: 1, 786: 1, 787: 1, 790: 1, 793: 1, 796: 1, 798: 1, 799: 1, 804: 1, 806: 1, 807: 1, 808: 1, 809: 2, 811: 1, 812: 1}}\n",
      "matches:\n",
      "{5: [96, 91, 3, 0, 0, 0], 343: [30, 24, 0, 0, 1, 0], 580: [46, 44, 6, 0, 1, 0], 861: [7, 6, 0, 0, 1, 3], 932: [53, 1, 0, 0, 0, 0], 712: [85, 87, 20, 23, 0, 11], 1195: [32, 35, 2, 1, 4, 6], 1277: [35, 3, 2, 1, 1, 1], 1284: [61, 72, 0, 0, 1, 14], 1360: [83, 1, 2, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"res:\")\n",
    "print(res)\n",
    "print(\"matches:\")\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# for key in rows:\n",
    "#     if rows.get(key) >= 2:\n",
    "#         qe = \"SELECT * FROM api_base WHERE proj_id= %s AND pub_id = %s\"\n",
    "#         de = (pjid, str(key))\n",
    "#         mycursor.execute(qe, de)\n",
    "#         print(mycursor.fetchall())\n",
    "#         counter +=1\n",
    "# print(\">= 2: \" + counter)\n",
    "# for key in rows:\n",
    "#     if rows.get(key) >= 3:\n",
    "#         qe = \"SELECT * FROM api_base WHERE proj_id= %s AND pub_id = %s\"\n",
    "#         de = (pjid, str(key))\n",
    "#         mycursor.execute(qe, de)\n",
    "#         print(mycursor.fetchall())\n",
    "#         counter +=1\n",
    "# print(\">= 3: \" + counter)\n",
    "# for key in rows:\n",
    "#     if rows.get(key) >= 4:\n",
    "#         qe = \"SELECT * FROM api_base WHERE proj_id= %s AND pub_id = %s\"\n",
    "#         de = (pjid, str(key))\n",
    "#         mycursor.execute(qe, de)\n",
    "#         print(mycursor.fetchall())\n",
    "#         counter +=1\n",
    "# print(\">= 4: \" + counter)\n",
    "# for key in rows:\n",
    "#     if rows.get(key) >= 5:\n",
    "#         qe = \"SELECT * FROM api_base WHERE proj_id= %s AND pub_id = %s\"\n",
    "#         de = (pjid, str(key))\n",
    "#         mycursor.execute(qe, de)\n",
    "#         print(mycursor.fetchall())\n",
    "#         counter +=1\n",
    "# print(\">= 5: \" + counter)\n",
    "# for key in rows:\n",
    "#     if rows.get(key) >= 6:\n",
    "#         qe = \"SELECT * FROM api_base WHERE proj_id= %s AND pub_id = %s\"\n",
    "#         de = (pjid, str(key))\n",
    "#         mycursor.execute(qe, de)\n",
    "#         print(mycursor.fetchall())\n",
    "#         counter +=1\n",
    "# print(\">= 6: \" + counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inputs'''\n",
    "pjid = 5\n",
    "year = 2001\n",
    "aut1 = \"J. Bradford Jensen\"\n",
    "aut2 = \"J. Jensen\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%foreign trade%\"\n",
    "kw2 = \"%U.S. economy%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%party transactions%\"\n",
    "kw4 = \"%domestic production%\"\n",
    "kw5 = \"%local labor market%\"\n",
    "kw6 = \"%FTD%\"\n",
    "ds1 = \"%Annual Survey of Manufactures%\"\n",
    "ds = ['Annual Survey of Manufactures', 'Auxiliary Establishment - ES9200', 'Census of Construction Industries',\n",
    "     'Census of Finance, Insurance, and Real Estate', 'Census of Manufactures', 'Census of Mining',\n",
    "     'Census of Retail Trade', 'Census of Services', 'Census of Transportation, Communications, and Utilities',\n",
    "     'Census of Wholesale Trade', 'Enterprise Summary Report - ES9100 (large company)', 'Standard Statistical Establishment Listing']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 343\n",
    "year = 2004\n",
    "aut1 = \"Gale A. Boyd\"\n",
    "aut2 = \"Gale Boyd\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%modal choice%\"\n",
    "kw2 = \"%product shipments%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%forecasting capability%\"\n",
    "kw4 = \"%shipment specific effects%\"\n",
    "kw5 = \"%CFS%\"\n",
    "kw6 = \"%NEMS%\"\n",
    "ds1 = \"%Annual Survey of Manufactures%\"\n",
    "ds = ['Annual Survey of Manufactures', 'Commodity Flow Survey', 'Census of Manufactures',\n",
    "     'Census of Mining', 'Census of Retail Trade', 'Census of Wholesale Trade',\n",
    "     'Standard Statistical Establishment Listing']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 580\n",
    "year = 2006\n",
    "aut1 = \"Jagadeesh M. Sivadasan\"\n",
    "aut2 = \"Jagadeesh Sivadasan\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%Micro-Productivity%\"\n",
    "kw2 = \"%Heterogeneity%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%IPO%\"\n",
    "kw4 = \"%product market performance%\"\n",
    "kw5 = \"%going public%\"\n",
    "kw6 = \"%data quality%\"\n",
    "ds1 = \"%Annual Capital Expenditures Survey%\"\n",
    "ds = ['Annual Capital Expenditures Survey', 'Annual Survey of Manufactures', 'Auxiliary Establishment - ES9200',\n",
    "     'Business Expenditures Survey', 'Census of Manufactures', 'Compustat',\n",
    "     'Census of Retail Trade', 'Longitudinal Business Database', 'Survey of Plant Capacity Utilization',\n",
    "     'Survey of Industrial Research and Development', 'Survey of Manufacturing Technology', 'Standard Statistical Establishment Listing']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 861\n",
    "year = 2011\n",
    "aut1 = \"Keren M. Horn\"\n",
    "aut2 = \"Keren Horn\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%neighborhood quality%\"\n",
    "kw2 = \"%household residential choices%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%neighborhood change%\"\n",
    "kw4 = \"%racial composition%\"\n",
    "kw5 = \"%zoned public school%\"\n",
    "kw6 = \"%crime rate%\"\n",
    "ds1 = \"%American Community Survey%\"\n",
    "ds = ['American Community Survey', 'American Housing Survey', 'Decennial Census']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 932\n",
    "year = 2012\n",
    "aut1 = \"William R. Walker\"\n",
    "aut2 = \"William Walker\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%positive shocks%\"\n",
    "kw2 = \"%prenatal conditions%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%LEHD%\"\n",
    "kw4 = \"%supplemental nutritional program%\"\n",
    "kw5 = \"%clean air act%\"\n",
    "kw6 = \"%fetal origins hypothesis%\"\n",
    "ds1 = \"%Current Population Survey%\"\n",
    "ds = ['Current Population Survey', 'Current Population Survey (March/ASEC)', 'LEHD Employment History File',\n",
    "     'LEHD Individual Characteristics File', 'Survey of Income and Program Participation']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 712\n",
    "year = 2009\n",
    "aut1 = \"Nicholas A. Bloom\"\n",
    "aut2 = \"Nicholas Bloom\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%uncertainty%\"\n",
    "kw2 = \"%business cycle%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%time series%\"\n",
    "kw4 = \"%volatility%\"\n",
    "kw5 = \"%nonresponse%\"\n",
    "kw6 = \"%cyclical%\"\n",
    "ds1 = \"%Annual Survey of Manufactures%\"\n",
    "ds = ['Annual Survey of Manufactures', 'Census of Manufactures', 'Compustat', \n",
    "     'Longitudinal Business Database', 'Survey of Plant Capacity Utilization', \n",
    "     'Survey of Industrial Research and Development', 'Standard Statistical Establishment Listing']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 1195\n",
    "year = 2014\n",
    "aut1 = \"Xavier A. Giroud\"\n",
    "aut2 = \"Xavier Giroud\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%resource allocation%\"\n",
    "kw2 = \"%business cycle%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%great recession%\"\n",
    "kw4 = \"%capital stock imputation%\"\n",
    "kw5 = \"%classication of establishments%\"\n",
    "kw6 = \"%datasets%\"\n",
    "ds1 = \"%Annual Survey of Manufactures%\"\n",
    "ds = ['Annual Survey of Manufactures', 'Auxiliary Establishment - ES9200', 'Census of Manufactures', \n",
    "     'Compustat', 'Longitudinal Business Database', 'LEHD Business Register Bridge', \n",
    "     'LEHD Employer Characteristics File', 'LEHD Employment History File', 'LEHD Geocoded Address List',\n",
    "     'LEHD Individual Characteristics File', 'LEHD Unit-to-Worker', 'Standard Statistical Establishment Listing']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 1277\n",
    "year = 2014\n",
    "aut1 = \"Richard Sander\"\n",
    "aut2 = \"R. Sander\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%labyrinth%\"\n",
    "kw2 = \"%housing segregation%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%residential segregation%\"\n",
    "kw4 = \"%housing legislation%\"\n",
    "kw5 = \"%black migration%\"\n",
    "kw6 = \"%schelling process%\"\n",
    "ds = ['American Community Survey', 'Decennial Census']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 1284\n",
    "year = 2015\n",
    "aut1 = \"Martha J. Bailey\"\n",
    "aut2 = \"Martha Bailey\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%long-run determinants%\"\n",
    "kw2 = \"%processes%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%ACS%\"\n",
    "kw4 = \"%location of birth%\"\n",
    "kw5 = \"%migration%\"\n",
    "kw6 = \"%education%\"\n",
    "ds = ['American Community Survey', 'Decennial Census', 'BOC Census Numident', 'Disclosure Avoidance Population Tables']\n",
    "\n",
    "'''inputs'''\n",
    "pjid = 1360\n",
    "year = 2015\n",
    "aut1 = \"Ann L. Owens\"\n",
    "aut2 = \"Ann Owens\"\n",
    "# two keywords from title to search in title and descriptions\n",
    "kw1 = \"%trends%\"\n",
    "kw2 = \"%hispanic neighborhood ascent%\"\n",
    "# four keywords/phrases from abstract to search in description\n",
    "kw3 = \"%socioeconomic ascent%\"\n",
    "kw4 = \"%quantitative analyses%\"\n",
    "kw5 = \"%metropolitan areas%\"\n",
    "kw6 = \"%demographic processes%\"\n",
    "ds = ['American Community Survey', 'Decennial Census']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = Base()\n",
    "# results = api.searchAll(0, '\"Jack Johnson\"')\n",
    "# for i in range(5):\n",
    "#     hits = results[1]['response']['numFound']\n",
    "#     title = str(results[1]['response']['docs'][i]['dctitle'])\n",
    "#     #doi = str(results[1]['data'][i]['_source']['doi'])\n",
    "#     #work_id = str(results[1]['data'][i]['_id'])\n",
    "#     authors = results[1]['response']['docs'][i]['dccreator']\n",
    "#     #citations = str(results[1]['data'][i]['_source']['citations'])\n",
    "#     pub_year = str(results[1]['response']['docs'][i]['dchdate'][:4])\n",
    "#     pub_month = str(results[1]['response']['docs'][i]['dchdate'][5:7])\n",
    "#     full_text_link = str(results[1]['response']['docs'][i]['dclink'])\n",
    "#     description = str(results[1]['response']['docs'][i]['dcdescription'])\n",
    "#     print(\"i: \"+str(i))\n",
    "#     print(\"hits: \"+str(hits))\n",
    "#     print(\"title: \"+title)\n",
    "#     print(\"authors: \")\n",
    "#     for author in authors:\n",
    "#         print(author)\n",
    "#     print(\"pub_year: \"+pub_year)\n",
    "#     print(\"pub_month: \"+pub_month)\n",
    "#     print(\"full_text_link: \"+full_text_link)\n",
    "#     print(\"description: \"+description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as urllib2\n",
    "\n",
    "# proxy = urllib2.ProxyHandler({\"https\": \"148.129.129.18:3128\"})\n",
    "# opener = urllib2.build_opener(proxy)\n",
    "# urllib2.install_opener(opener)\n",
    "# page = urllib2.urlopen(\"https://api.base-search.net/cgi-bin/BaseHttpSearchInterface.fcgi?func=PerformSearch&query=%22Ronald%20Jarmin%22&format=json\")\n",
    "# print(page.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
